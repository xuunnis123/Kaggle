# -*- coding: utf-8 -*-
"""Lab3_Freshlight.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kiu6Scg3P2SFIKvbsUhIYI-mIjMCcohG
"""

ls drive/MyDrive/'Colab Notebooks'/input

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector
from keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
import sklearn.preprocessing

import seaborn as sns
# %matplotlib inline
np.random.seed(2)

pd.set_option('display.max_columns', None) # Show all columns
train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')
holidays_events = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/holidays_events.csv')
oil = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/oil.csv')
stores = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/stores.csv')
transactions = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/transactions.csv').sort_values(["store_nbr", "date"])
# Convert to datetime type
train["date"] = pd.to_datetime(train.date)
test["date"] = pd.to_datetime(test.date)

train # date 20130101~20170815, store_nbr 55 types, family 33 types, 3000888 raws

test # date 20170816~20170831 28512 raws

# Check train & test data havn't null value
train_null = train.isnull().sum()
test_null = test.isnull().sum()
print(train_null, test_null, sep='\n\n')

round(train.iloc[:,1:].describe(),2)

print('The number of the family: {}'.format(len(train['family'].unique())))
print(train['family'].unique())
print()
print('The number of the store_nbr: {}'.format(len(train['store_nbr'].unique())))
print(train['store_nbr'].unique())

"""## What does data look like?"""

def show_data(data, title):
  print('==={}==='.format(title))
  print(data.head(5))
  print()
show_data(holidays_events, 'Holidays_events') # Pay special attention to the transferred column
show_data(oil, 'Oil')
show_data(stores, 'Stores')
show_data(transactions, 'Transactions')
# Holidays_events is so complicated that we would not use it

"""## Oil"""

import matplotlib.ticker as ticker
oil['date'] = pd.to_datetime(oil.date)
plt.title("Oil price trend")
plt.xlabel("Date")
plt.ylabel("Oil price")
sns.lineplot(data=oil, x="date", y="dcoilwtico")
# Oil prices are relatively stable compared to previous years

"""## Transactions"""

transactions['date'] = pd.to_datetime(transactions.date)
f, axes = plt.subplots(1, 2, figsize=(20,8))
sns.lineplot(data=transactions, x = 'date', y = 'transactions', hue = 'store_nbr',palette='rainbow', ax=axes[0])
sns.scatterplot(data=transactions, x = 'date', y = 'transactions', hue = 'store_nbr',palette="ch:r=-.2,d=.3_r", ax=axes[1])
# Every December is the peak period of trading

train_a_month = transactions.loc[transactions.date >= '2017-07-16']
f, axes = plt.subplots(figsize=(20,8))
train_a_month['date'] = train_a_month.date.dt.strftime('%m/%d/%A')
axes.set_xticklabels(labels=train_a_month['date'], rotation=45)
sns.scatterplot(data=train_a_month, x = 'date', y = 'transactions', hue = 'store_nbr',palette="ch:r=-.2,d=.3_r", ax=axes)
# x_dates = train_a_month['date'].dt.day_name()
# print(x_dates)
# axes.set_xticklabels(labels=x_dates)
# Compared to weekdays, weekends have more transactions

"""## Stores"""

colors = sns.color_palette('pastel')
stores_1 = stores['type'].value_counts().rename_axis('Type').to_frame('Count').sort_values(by = 'Type')
stores_2 = stores[['type','cluster']].groupby('type').sum()
stores_show = pd.merge(stores_1, stores_2, left_index=True, right_index=True)
plt.pie(stores_show['Count'], labels = stores_show.index, colors = colors, autopct='%.0f%%')
plt.axis('equal')
plt.title('Stores type piechart')
stores_show

trade_a_month = pd.merge(transactions.loc[transactions.date >= '2017-07-16'], stores[['store_nbr','type','cluster']]).sort_values(['date', 'store_nbr'])
trade_a_month
f, axes = plt.subplots(figsize=(20,8))
sns.lineplot(data=trade_a_month, x = 'date', y = 'transactions', hue = 'type',palette='rainbow', ax=axes)
# Type A has the most numbers of trancations in last month

"""## Model

## Average the number of transactions in the last three weeks to predict
"""

import numpy  as np
import pandas as pd


# create a 'day of the week' feature
train['day_of_the_week'] = train['date'].dt.day_name()
test['day_of_the_week']  = test['date'].dt.day_name()

# select the very last three weeks of the training data
train_three_weeks = train.query("date >= '2017-07-26' ")

def exp_mean_ln(df):
    return np.expm1(np.mean(np.log1p(df['sales'])))

# calculate the average values
train_average = train_three_weeks.groupby(['store_nbr','family','day_of_the_week']).apply(exp_mean_ln).to_dict()
test['sales'] = test.set_index(['store_nbr','family','day_of_the_week']).index.map(train_average.get)

# create and write out the submission.csv file
submission = pd.DataFrame({'id': test.id, 'sales': test.sales})
submission.to_csv('submission.csv', index=False)

"""## RNN"""

train = train.query("date>'2017-6-1'")
tmp_train = train.groupby('family')
tmp_test = test.groupby('family')

def build_to_train_data(train_df,pDays=60,fDays=16):
    x_train = []
    y_train = []

    for i in range(len(train_df)-pDays-fDays):
        x_train.append(np.array(train_df[i:i+pDays]))
        y_train.append(np.array(train_df[i+pDays:i+pDays+fDays]))
    return np.array(x_train), np.array(y_train)

def shuffle(X,Y):
    np.random.seed(10)
    randomList = np.arange(X.shape[0])
    np.random.shuffle(randomList)
    return X[randomList], Y[randomList]

def build_model(shape):
    model = Sequential()
    model.add(LSTM(50, input_length=shape[1], input_dim=shape[2]))
    #
    model.add(RepeatVector(2))
    model.add(LSTM(50))
    #
    
    model.add(RepeatVector(2))
    model.add(LSTM(50))
    # output shape: (1, 1)
    model.add(Dense(54))
    model.compile(loss="mse", optimizer="adam")
    model.summary()
    return model

result = {}
pass_days = 3

for family in tmp_train.groups.keys():
    df1 = tmp_train.get_group(family)
    tmp_df2 = pd.DataFrame(columns=list(range(1,55)))
    
    for i in df1['date'].unique():
        filter = df1['date']==i
        tmp_df1 = df1[filter].sort_values('store_nbr').set_index('store_nbr')
        
        tmp_df2.loc[i] = list(tmp_df1['sales'])

    sc = MinMaxScaler(feature_range = (0, 1))

    X, Y = build_to_train_data(sc.fit_transform(tmp_df2), pass_days,1)
    X, Y = shuffle(X,Y)

    X_train = X[int(len(X)*0.1):]
    Y_train = Y[int(len(X)*0.1):]
    X_val = X[:int(len(X)*0.1)]
    Y_val = Y[:int(len(X)*0.1):]

    model = build_model(X_train.shape)
    callback = EarlyStopping(monitor="loss", patience=10, verbose=1, mode="auto")
    model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])
    

    test_df = tmp_df2[pass_days*-1:]
    for i in range(16):
        x_test = []
        x_test.append(np.array(test_df[i:i + pass_days]))
        ans = model.predict(np.array(x_test))

        tmp_time = test_df.index[-1] + timedelta(days=1)
        test_df.loc[tmp_time] = list(ans[0])
    
    tarr = test_df[-16:]
    tdf = pd.DataFrame(sc.inverse_transform(tarr),index=list(test_df[-16:].index),columns=list(test_df[-16:].head(0)))
    rdf = test.groupby('family').get_group(family)


    for i in range(len(rdf)):
        id = rdf.iloc[i]['id']
        date = rdf.iloc[i]['date']
        store_nbr = rdf.iloc[i]['store_nbr']
        result[id] = tdf.loc[date][store_nbr]

result_dataframe = pd.DataFrame(list(result.items()),columns=['id', 'sales'])
final_result = result_dataframe.sort_values('id')
final_result[final_result<0] = 0

final_result.to_csv("submission.csv",index=False)
final_result