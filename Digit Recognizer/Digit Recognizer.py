# -*- coding: utf-8 -*-
"""Lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vRAoiL1oN3ulPrpXFsh3zx5MePTHz9Tn

# Use MLP to predict the results
"""

from google.colab import drive
drive.mount('/content/drive')

# from keras.models import Sequential
# from keras.utils import np_utils
# from keras.layers.core import Dense, Activation, Dropout

# import pandas as pd
# import numpy as np

# # Read data
# train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')
# labels = train.iloc[:,0].values.astype('int32')
# X_train = (train.iloc[:,1:].values).astype('float32')
# X_test = (pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test.csv').values).astype('float32')

# # convert list of labels to binary class matrix
# y_train = np_utils.to_categorical(labels) 

# # pre-processing: divide by max and substract mean
# scale = np.max(X_train)
# X_train /= scale
# X_test /= scale

# mean = np.std(X_train)
# X_train -= mean
# X_test -= mean

# input_dim = X_train.shape[1]
# nb_classes = y_train.shape[1]

# # Here's a Deep Dumb MLP (DDMLP)
# model = Sequential()
# model.add(Dense(128, input_dim=input_dim))
# model.add(Activation('relu'))
# model.add(Dropout(0.15))
# model.add(Dense(128))
# model.add(Activation('relu'))
# model.add(Dropout(0.15))
# model.add(Dense(nb_classes))
# model.add(Activation('softmax'))

# # we'll use categorical xent for the loss, and RMSprop as the optimizer
# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# print("Training...")
# model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.1, verbose=2)

# print("Generating test predictions...")
# preds = model.predict(X_test, verbose=0)
# classes_x=np.argmax(preds,axis=1)

# def write_preds(preds, fname):
#     pd.DataFrame({"ImageId": list(range(1,len(preds)+1)), "Label": classes_x}).to_csv(fname, index=False, header=True)

# write_preds(preds, "keras-mlp.csv")
# print("Done!")

"""# Use CNN to predict the results

## Import necessary tools
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
# Could omit plt.show()
# %matplotlib inline 

np.random.seed(2)

from sklearn.model_selection import train_test_split

from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D

"""## Data Preparation"""

# Read data
train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test.csv')

x_train = train.iloc[:,1:]
y_train = train['label']

sns.countplot(y_train) # The data labels are Evenly distributed
y_train.value_counts()

# Check Train data set hasn't include Null value
x_train.isnull().any().describe()

# Check Test data set hasn't include Null value
test.isnull().any().describe()

"""## Use Normalization & Reshape"""

x_train /= 255
test /= 255
x_train = x_train.values.reshape(-1,28,28,1)
test = test.values.reshape(-1,28,28,1)
x_train.shape # (42000,784) -> (42000,28,28,1)

"""## Let label to be a one hot vector"""

y_train = to_categorical(y_train, num_classes = 10) # 0 ~ 9
y_train

"""## Training Phase"""

random_seed = 2
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=random_seed) # From Sklearn

x_train[0].shape
f, axarr = plt.subplots(3,3,figsize=(9, 9))
# Show some images in training data
for i in range(3):
  for j in range(3):
    axarr[i,j].imshow(x_train[i*10+j][:,:,0])

model = Sequential()
# Model Structure: [[Conv + Relu]*2 -> MaxPool -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Classification
# Parameters Setting
# filters ususally are the multiple of four
# padding="same" results in padding with zeros ,so the input dim could equal to the output dim
# Second best model -> filters = 32, kernel_size(5,5)
model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2))) # use MaxPool2D to find the maximum value and get the feature maps with dimension reduction
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation = "relu"))
model.add(Dropout(0.5))
model.add(Dense(10, activation = "softmax"))

# In order to reduce the learning rate
from keras.callbacks import ReduceLROnPlateau
LR_function=ReduceLROnPlateau(monitor='val_accuracy',
                             patience=3, # Modified LR when val_accuracy not declined within 3 epochs
                             verbose=1,
                             factor=0.5, # LR reduce to 0.5
                             min_lr=0.00001 # This is the minimum LR
                             )

# We'll use categorical_crossentropy for the loss, and RMSprop as the optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# Epochs 40 batch_size 100 best accuracy 0.99125 -> 0.99203
epochs = 40
batch_size = 100
#history = model.fit_generator(x_train, y_train, epochs=epochs, validation_data=(x_val,y_val), verbose=2, steps_per_epoch=x_train.shape[0] // batch_size)
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val,y_val), verbose=2, callbacks=[LR_function])

# Plot the loss and accuracy curves for training and validation 
fig, ax = plt.subplots(2,1,figsize=(8,6))
ax[0].plot(history.history['loss'], color='b', label="Training loss")
ax[0].plot(history.history['val_loss'], color='r', label="Validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True, fontsize="large") # Set fontsize="large" to avoid the letter t & r  overlap

ax[1].plot(history.history['accuracy'], color='b', label="Training accuracy")
ax[1].plot(history.history['val_accuracy'], color='r',label="Validation accuracy")
legend = ax[1].legend(loc='best', shadow=True, fontsize="large") # Set fontsize="large" to avoid the letter t & r  overlap

# predict results
results = model.predict(test)
print(results)
# select the indix with the maximum probability
results = np.argmax(results,axis = 1)
print(results)
results = pd.Series(results,name="Label")

submission = pd.concat([pd.Series(range(1,28001),name = "ImageId"),results],axis = 1)
print(submission)
submission.to_csv("cnn_mnist_datagen.csv",index=False)