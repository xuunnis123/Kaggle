# -*- coding: utf-8 -*-
"""Lab1_Freshlight.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oapVDJBjnU7fP45qjgr8oupyHBtR-1Jh

# Import Library & Read CSV & Data Analysis
"""

from google.colab import drive
drive.mount('/content/drive')

import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# First replace the id with the PassengerId 
gender_submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/gender_submission.csv', index_col= ['PassengerId'])

# Load the train, test, gender_submission file to dataframes
train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv', index_col= ['PassengerId'])
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test.csv',  index_col= ['PassengerId'])
gender_submission  # This is sample that all woman are survied, and all man are unsurvived

train.head(15) #Training Data (Pclass => Ticket Class; SibSp => Siblings, Spouses; Parch => Parents, Children; Embarked => Port)

test #Data without Survived Column

desc = train.describe()
desc.loc['count'] = desc.loc['count'].astype(int) #let type(count) = int
desc.iloc[1:] = desc.iloc[1:].applymap('{:.2f}'.format) # set float to the second digit after the decimal point
desc #There are only 38% people survived

unique, count= np.unique(train["Survived"], return_counts=True) # Results: unique = [0, 1], count = [549, 342]
print("Died : %s, Survived : %s " % (count[0], count[1]))
print("Total Passengers in Training : %s, All kinds of features : %s" % (train.shape[0], train.shape[1]))
train_na = train.isna().sum() # Check whether row data in train have a null value or not
train_na[lambda x: x != 0] # Missing info in some Training data, so we're gonna predict the info.

f, ax = plt.subplots(1,2,figsize=(10, 8))
# Using matplotlib library
ax[0].matshow(train.corr())

# Using seaborn library
sns.heatmap(train.corr(), mask=np.zeros_like(train.corr(), dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax[1])

plt.tight_layout()
plt.show()
# We can see that there is a negative correlation between Pclass & fare 
train.corr()

"""# Data Cleaning
- Name : Extract name, only keep title
- Age, Cabin, Embarked have **nan value** in rows, so we need to set values for them.

## Data Cleaning : Name
"""

train['Extract_name'] = train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())
most_frequency = list(train['Extract_name'].value_counts()[:5].index) # Top 5 most frequency Title, we called others "Others"
print(most_frequency)
train.loc[~train["Extract_name"].isin(most_frequency), 'Extract_name'] = 'Others'
train.loc[:,['Name','Extract_name']] # Now we could use more concise way to present Name

# Show that number of individual names & survived rates
fig, ax = plt.subplots(1,2,figsize = (15,6))
sns.countplot( x='Extract_name', data=train, palette='Set2', ax = ax[0], order = ['Mr','Miss','Mrs','Master','Dr','Others'])
sns.barplot( x='Extract_name', y='Survived', data=train, palette='Set2', ax = ax[1], order = ['Mr','Miss','Mrs','Master','Dr','Others'])
train['Extract_name'] = train['Extract_name'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Dr': 4, 'Others': 5})

"""## Data Cleaning : Age"""

fig, axes = plt.subplots(1,1,figsize=(10,6))
sns.histplot(train['Age'][(train['Age'].notnull())&(train['Survived']==0)] , kde=True, color='r', ax=axes )
sns.histplot(train['Age'][(train['Age'].notnull())&(train['Survived']==1)] , kde=True, color='b', ax=axes )
plt.legend(['Not Survived', 'Survived'])# People between 20 and 40 are at high risk of death

# Previously we saw that we had 177 rows of Age are null, so we give the mean value of Age to them
def Age_classify(age):
  return int(age // 10)
train['Age'].fillna(value = train['Age'].mean(),inplace = True) # Set the null value to  Age's mean value
train['Age'].isna().sum() # Everyone has Age right now!
train['Age'] = train['Age'].apply(Age_classify)

# Show that number of individual Age's classes & survived rates
fig, ax = plt.subplots(1,2,figsize = (15,6))
sns.countplot( x='Age', data=train, palette='Paired', ax = ax[0])
sns.barplot( x='Age', y='Survived', data=train, palette='Paired', ax = ax[1])

"""## Data Cleaning : Cabin"""

train['Cabin'].value_counts() # Only keep letter
def Keep_letter(letter):
  if type(letter) == float: return 'N' # This is nan condition
  else:
    return letter[0]
train['Cabin'] = train['Cabin'].apply(Keep_letter)

df = pd.concat([train['Cabin'], test['Cabin']])
df = df.apply(Keep_letter)
fig, ax = plt.subplots(1,2,figsize = (15,6))
sns.countplot( x=df, data=train, palette='rainbow', ax = ax[0])
sns.barplot( x=df, y='Survived', data=train, palette='rainbow', ax = ax[1])
# The result shows that we could split Cabin to five pieces

# Show that number of individual Cabin's classes & survived rates
train['Cabin'] = train['Cabin'].map({'N': 0, 'T': 0, 'C': 1, 'F': 1, 'E': 2, 'D': 2, 'B': 2, 'G': 3, 'A': 3})
fig, ax = plt.subplots(1,2,figsize = (15,6))
sns.countplot( x='Cabin', data=train, palette='rainbow', ax = ax[0])
sns.barplot( x='Cabin', y='Survived', data=train, palette='rainbow', ax = ax[1]) # Simplify the problem to Five kinds in the Cabin

"""## Data Cleaning : Embarked"""

fig, ax = plt.subplots(1,2,figsize = (10,6))
sns.countplot(x='Embarked', data=train, palette='rainbow', ax = ax[0]) # To see which port has the most people embarked
sns.barplot( x='Embarked', y='Survived', data=train, palette='rainbow', ax = ax[1])
train['Embarked'] = train['Embarked'].map({"S": 0,"C": 1,"Q": 2})
train['Embarked'].fillna(value = 0, inplace = True) # Set the null value to  0 (Common port)

"""## Data cleaning : Fare"""

# We are not sure how to deal with the fare, so we didn't use that
# Processing: 
#   1.Set all value in Fare to the logarithm of themselves
#   2.Split Fare to four classes
train['Fare'].value_counts()
train['Fare'] = np.log(train['Fare'])
for i in train['Fare'].index:
  if train['Fare'][i] < 2 : train['Fare'][i] = 0
  elif train['Fare'][i] < 3 : train['Fare'][i] = 1
  elif train['Fare'][i] < 4 : train['Fare'][i] = 2
  else: train['Fare'][i] = 3
fig, ax = plt.subplots(1,2,figsize = (10,6))
sns.countplot(x='Fare', data=train, palette='rainbow', ax = ax[0])
sns.barplot( x='Fare', y='Survived', data=train, palette='rainbow', ax = ax[1])

"""## Data cleaning: Sibsp + Parch"""

train['Family'] = train['Parch'] + train['SibSp'] + 1 # Including own self
train.loc[train['Family'] >= 5, 'Family'] = 4
fig, ax = plt.subplots(1,2,figsize = (15,6))
sns.countplot( x='Family', data=train, palette='Spectral', ax = ax[0])
sns.barplot( x='Family', y='Survived', data=train, palette='Spectral', ax = ax[1])
# The result shows that we could split Cabin to five pieces

# We use one hot vector on Pclass, but its result was not good
# train = train.join(pd.get_dummies(train['Pclass'], prefix='Pclass_')) 
train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})
Y_train = train[["Survived"]].values
train.drop(['Name', 'Fare','Ticket', 'Survived', 'Parch', 'SibSp'], axis = 1, inplace = True)# Maybe useless(?

X_train = train.values

# Change type to int
train['Embarked'] = train['Embarked'].astype(int)
train['Cabin'] = train['Cabin'].astype(int)
train

"""# Data Preprocessing: Test"""

# Do the same thing, Data cleaning and Engineer
# Name -> Extract_name
test['Extract_name'] = test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())
test.loc[~test["Extract_name"].isin(most_frequency), 'Extract_name'] = 'Others'
test.loc[:,['Name','Extract_name']] # Now we could use more concise way to present Name
test['Extract_name'] = test['Extract_name'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Dr': 4, 'Others': 5})
# Age
test['Age'].fillna(value = test['Age'].mean(),inplace = True)
test['Age'] = test['Age'].apply(Age_classify)
# Cabin
test['Cabin'] = test['Cabin'].apply(Keep_letter)
test['Cabin'] = test['Cabin'].map({'N': 0, 'T': 0, 'C': 1, 'F': 1, 'E': 2, 'D': 2, 'B': 2, 'G': 3, 'A': 3})
# Embarked
test['Embarked'] = test['Embarked'].map({"S": 0,"C": 1,"Q": 2})
test['Embarked'].fillna(value = 0, inplace = True)

#test = test.join(pd.get_dummies(test['Pclass'], prefix='Pclass_'))
# Sex
test['Sex'] = test['Sex'].map({'male': 0, 'female': 1})
# Sibsp, Parch -> Family
test['Family'] = test['Parch'] + test['SibSp'] + 1 # Including own self
test.loc[test['Family'] >= 5, 'Family'] = 4
# Fare
# test['Fare'] = np.log(test['Fare'])
# for i in test['Fare'].index:
#   if test['Fare'][i] < 2 : test['Fare'][i] = 0
#   elif test['Fare'][i] < 3 : test['Fare'][i] = 1
#   elif test['Fare'][i] < 4 : test['Fare'][i] = 2
#   else: test['Fare'][i] = 3

test.drop(['Name','Fare','Ticket', 'Parch', 'SibSp'], axis = 1, inplace = True)# Maybe useless(?
X_test = test.values

test['Embarked'] = test['Embarked'].astype(int)
test['Cabin'] = test['Cabin'].astype(int)

X_test = preprocessing.scale(X_test)
X_test.shape
test.isna().sum()

"""# Model Training"""

# Use keras.models.Sequential()
# Build, compile and fit the model
from keras.layers import *
from keras.models import Sequential
from tensorflow.keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf 

# Standardize data for MLP
from sklearn import preprocessing

# Fixed the result
tf.random.set_seed(69)

X_train = preprocessing.scale(X_train)
print(X_train)

# Try and Error!
model = Sequential()
model.add(Dense(16, activation = "relu", input_dim = X_train.shape[1]))
#model.add(Dropout(0.1))
model.add(Dense(16, activation = "relu"))
#model.add(Dropout(0.1))
#model.add(Dense(16, activation = "relu"))
#model.add(Dropout(0.1))
model.add(Dense(1, activation = "sigmoid"))
# Hyperparameter 
epochs = 50
#lr = 0.01
#decay = lr / epochs
#momentum = 0.8

# Preset of Opt is better than we tried
#opt = optimizers.Adam(lr = 0.001,decay = 0, beta_1=0.9, beta_2=0.999, epsilon = None, amsgrad = True)
#opt = optimizers.SGD(lr=lr, decay=decay, momentum=momentum, nesterov=False)

# training
model.compile(loss = "binary_crossentropy", optimizer = 'Adam', metrics = ["accuracy"])

# track model performance, we also tried earlystop and checkpoint
stop = EarlyStopping(monitor = 'val_loss', patience = epochs, verbose = 1)
filePath = '/content/drive/MyDrive/Colab Notebooks/tmp/weights.hdf5'
checkpointer = ModelCheckpoint(filepath= filePath, monitor = "val_loss", mode = 'min',verbose=1, save_best_only=True)

history = model.fit(X_train, Y_train, epochs = epochs, validation_split = 0.33, shuffle = True, 
        verbose = 1, batch_size = 8)
Loss, Accurancy = model.evaluate(X_train, Y_train)  
print("Loss: {0}, Accurancy: {1}".format(Loss, Accurancy)) # Performance 
#model.load_weights(checkpointer)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""# Data Prediction"""

# Sigmoid activation gives probabilities of 1 (Survived) 
# Binarizer turns this into appropiate output for competition guidelines
predictions = model.predict(X_test, batch_size=8)
print(X_test)
from sklearn.preprocessing import Binarizer
binarizer = Binarizer(threshold = 0.5)
predictions = binarizer.fit_transform(predictions)
predictions = predictions.astype(int)                       

# Create submission file
PassengerId = test.index
evaluation = PassengerId.to_frame()
evaluation["Survived"] = predictions
evaluation.to_csv("evaluation_submission.csv", index = False)